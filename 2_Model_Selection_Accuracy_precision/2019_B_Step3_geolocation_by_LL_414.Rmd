---
title: "Elemental map"
author: "Neil"
date: "17 March 2017"
output:
    ]
  html_document: 
  pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
editor_options: 
  chunk_output_type: console
---

#Setup
code not shown
import data, set hooks for webgl, subset edges and times, remove any 'n' using 'use column, make sure time points are correct
```{r include=FALSE}


rm(list=ls())
# getwd tells us where R is looking
getwd()
#setwd tells R where to look
#setwd('/Volumes/PHD/R_script/Microchemistry/Dec2016')


setwd('F:/R_script/Microchemistry/Elemental_maps')

dat<- read.csv('chem414.csv')
library(AID)
library(vegan)
library(MASS)
library(corrplot)
library(fields)
library(PMCMR)
library(gamm4)
library(GISTools)
library(SDMTools)
library(maps)
library(mapdata)
library(PBSmapping)
library(plyr)
library(raster)
library(rgdal)
library(spatial.tools)
library(RColorBrewer)
library(rgeos)
library(rasterVis)
library(rgl)
library(gstat)
library(sampling)
library(pROC)
library(flux)

source("sdm_functions.R")

## spatial extent etc set-up
# 
full.ext<- extent(c(-8, -3, 52, 60))
###coord system to assign
WGS84<- '+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0'
#### projection
mrc <- '+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs'
bathT<- raster('F:/R_script/SDM/full_area_time/layers_output/bath.grd')
base<- readOGR(dsn='F:/R_script/SDM', layer="full")

land<- readOGR(dsn= "F:/R_script/SDM/my_layers_collection", layer ='Britain_Proj_Dist')
land<- spTransform(land, CRS(WGS84))
land<- crop(land, full.ext)
base<- crop(base, full.ext)


landT<- spTransform(land, CRS(mrc))
baseT<- spTransform(base, CRS(mrc))
bathT<- crop(bathT, landT)# crop this here as it has already been transformed

# dat<- subset(dat, dat$use=='y')
# summary(dat$Area)
# # subset for the time points we want
# # and get the edge data
# edge<- subset(dat, dat$Reverse_pit_no=='1')
# edge$Mn_0<- as.numeric(as.character(edge$Mn_0))
# 
# ed1.14<- subset(edge, edge$sample_time ==1.14)
# ed1.14<- droplevels(ed1.14)
# 
# ed4.14<- subset(edge, edge$sample_time ==4.14)
# ed4.14<- droplevels(ed4.14)
# 
# ed1.15<- subset(edge, edge$sample_time ==1.15)
# ed1.15<- droplevels(ed1.15)
# 
# #make sure we are looking at the correct months
# ##### 1.14
# summary(ed4.14$Sample_months)
# summary(ed4.14$pred_month)
# 
# 
# ##### 4.14
# summary(ed4.14$Sample_months)
# summary(ed4.14$pred_month)
# ## remove ones which are way too early
# ed4.14<- subset(ed4.14, ed4.14$pred_month != 'Aug' & ed4.14$pred_month != 'Jul')
# summary(ed4.14$pred_month)
# ed4.14<- droplevels(ed4.14)
# str(ed4.14)
# ed4.14<- ed4.14[-57,]
# 
# ##### 1.15
# summary(ed1.15$Sample_months)
# summary(ed1.15$pred_month)
#region levels -  66-outer clyde
#                 79 - N Minch
#                 92 - offshore
#                 105 - S Minch
#                 110 - E IS
#                 112 - W IS
#                 115 - inner clyde
table(dat$HaulF, dat$Area)
region<- dat$Area

#region<-as.factor(dat$Haul_id)
# levels(region) <- c("Cl",  
#                     "Scot", 
#                     "Scot",
#                     "Scot", 
#                     "EIS",
#                     "WIS", "Cl")
 ed4.14<-cbind(dat, region)

```

# pull in surfaces and er
```{r}

# smoothed rasters
#normal
Na<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/modelNa_414.gri')
Na.er<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/er_modelNa_414.gri')

Mg<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/modelMg_414.gri')
Mg.er<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/er_modelMg_414.gri')

P<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/modelP_414.gri')
P.er<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/er_modelP_414.gri')

Ba<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/modelBa_414.gri')
Ba.er<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/er_modelBa_414.gri')

Sr<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/modelSr_414.gri')
Sr.er<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/er_modelSr_414.gri')

Mn<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/modelMn_414.gri')
Mn.er<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/er_modelMn_414.gri')

Zn<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/modelZn_414.gri')
Zn.er<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/er_modelZn_414.gri')

Rb<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/modelRb_414.gri')
Rb.er<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/er_modelRb_414.gri')

Li<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/modelLi_414.gri')
Li.er<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/er_modelLi_414.gri')

Sc<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/modelSc_414.gri')
Sc.er<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/er_modelSc_414.gri')






```

```{r}
# 
# ## pull a sample row from data
# samp<- ed4.14[sample(nrow(ed4.14), 1), ]
# answer= samp$Area # save the answer row for later
# 
# # make sample into an sp object and project
# coords.tmp<- cbind(samp$Mid_long, samp$Mid_lat)
# ## create spdf object
# samp.spdf<- SpatialPointsDataFrame(coords.tmp, data = data.frame(samp),
#                                    proj4string = CRS(WGS84))
# samp.spdfT<- spTransform(samp.spdf, CRS(mrc))
# 
# 
# 
# 
# # Pull in the data and make it larger sclae to speed up the process
# 
# #Na3d.big<-aggregate(Na3d, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# #Na3der.big<-aggregate(Na3der, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# 
# distNa.big<-aggregate(distNa, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# distNaer.big<-aggregate(distNaer, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# 
# distMg.big<-aggregate(distMg, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# distMger.big<-aggregate(distMger, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# 
# distP.big<-aggregate(distP, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# distPer.big<-aggregate(distPer, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# 
# distBa.big<-aggregate(distBa, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# distBaer.big<-aggregate(distBaer, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# 
# distSr.big<-aggregate(distSr, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# distSrer.big<-aggregate(distSrer, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# 
# distMn.big<-aggregate(distMn, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# distMner.big<-aggregate(distMner, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# 
# distCu.big<-aggregate(distCu, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# distCuer.big<-aggregate(distCuer, fact=4, fun=mean, expand=TRUE, na.rm=TRUE)
# 
# 
# stk<-stack(distNa.big, distNaer.big, distMg.big, distMger.big, distP.big, distPer.big, distBa.big, distBaer.big, distSr.big, distSrer.big, distMn.big, distMner.big, distCu.big, distCuer.big) # make a raster stack form the above
# 
# 
# param<- as.data.frame(stk, xy=TRUE)# change the above raster back to a dataframe to compute ll
# names(param)[3:16]<- c('meanNa', 'sdNa', 'meanMg', 'sdMg', 'meanP', 'sdP', 'meanBa', 'sdBa', 'meanSr', 'sdSr', 'meanMn', 'sdMn', 'meanCu', 'sdCu')
# 
# param<- cbind(param, ll=NA) # add the log likelyhood column
# head(param)
# 
# 
# 
# 
# ## log likelihood function for normal elements
# ll.fun <- function(parameters, thedata)
#   {
#   ll <- (dnorm(thedata, mean=parameters[1], sd=parameters[2], log=TRUE))
#   return(ll)
# }
# 
# 
# # a loop to estimate liklihood of the data given the parameters and add it to the ll column of param
# 
# for(i in 1:nrow(param))
#   
#  
# {
#   Na.param <- ll.fun(parameters=c(param$meanNa[i],param$sdNa[i]), thedata=samp$Na)
#   Mg.param <- ll.fun(parameters=c(param$meanMg[i],param$sdMg[i]), thedata=samp$Mg)
#   P.param <- ll.fun(parameters=c(param$meanP[i],param$sdP[i]), thedata=samp$P)
#   Ba.param <- ll.fun(parameters=c(param$meanBa[i],param$sdBa[i]), thedata=samp$Ba)
#   Sr.param <- ll.fun(parameters=c(param$meanSr[i],param$sdSr[i]), thedata=samp$Sr)
#   Mn.param <- ll.fun(parameters=c(param$meanMn[i],param$sdMn[i]), thedata=samp$Mn)
#   Cu.param <- ll.fun(parameters=c(param$meanCu[i],param$sdCu[i]), thedata=samp$Cu)
#   
#   param[i,17]<- (Na.param + Mg.param+ P.param+ Ba.param+ Sr.param + Mn.param + Cu.param)
# }
# 
# head(param)
# 
# image(distNa.big, main=' an example of one elements means')
# ##turn ll into sp object
# coords.tmp<- cbind(param$x, param$y)
# ## create spdf object
# ll.spdf<- SpatialPointsDataFrame(coords.tmp, data = data.frame(param),
#                                    proj4string = CRS(mrc))
# #then rasterise and the ll sp object
# ll.r<- raster(distNa.big)
# ll.r<- rasterize(ll.spdf, ll.r, field = 'll', fun = mean)
# 
# #plot the likelihood surface
# image(ll.r, main='maximum likelihood surface, circle =highes point, dot = true position')
# points(subset(ll.spdf, ll.spdf$ll==max(ll.spdf$ll,na.rm = TRUE)),cex=6) # add circle for highest point
# 
# 
# corr<- subset(ll.spdf, ll.spdf$ll==max(ll.spdf$ll,na.rm = TRUE))
# corr.r<- rasterize(corr, ll.r, field='meanNa', fun=mean)
# samp.r<- rasterize(samp.spdfT, ll.r, field='Na', fun=mean)
# 
# #image(corr.r, add=T, col ='black') #plot the guess
# image(samp.r, add=T, col = 'blue') # plot the truth
# 
# 
# dist(rbind(corr@coords,samp.spdfT@coords), method = "euclidean")
# 
# 
# 
# 
# answer
# 
# 
# # 
# 
# 


```

# add in measurment error to error surface
```{r}
par(mfrow=c(1,1))

Napool<- sqrt(Na.er+24.93217)
Mgpool<- sqrt(Mg.er+48.21544)
Ppool<- sqrt(P.er+690.9266)
Bapool<- sqrt(Ba.er+1.4901)
Srpool<- sqrt(Sr.er+1.40914)
Mnpool<- sqrt(Mn.er+0.6332)
Znpool<- sqrt(Zn.er+7.0531)
Rbpool<- sqrt(Rb.er+301.1896)
Lipool<- sqrt(Li.er+1.425543)

plot(Napool)
plot(Mgpool)
plot(Ppool)
plot(Bapool)
plot(Srpool)
plot(Mnpool)
plot(Znpool)
plot(Rbpool)
plot(Lipool)


```










#LL All Areas
```{r}




par(mfrow=c(1,1))

#### try by region
summary(ed4.14$region)
#ed4.14<- subset(ed4.14, ed4.14$region =='Cl')
#ed4.14<- subset(ed4.14, ed4.14$region =='EIS')
#ed4.14<- subset(ed4.14, ed4.14$region =='WIS')
#ed4.14<- subset(ed4.14, ed4.14$region =='Offshore')
#ed4.14<- subset(ed4.14, ed4.14$region =='Inshore')

mid= cbind(x=-722018.2, y=7704320) # mid inshore coords

# ed4.14<- subset(ed4.14, ed4.14$region !='EIS')
# ed4.14<- subset(ed4.14, ed4.14$region !='WIS')
#ed4.14<- subset(ed4.14, ed4.14$region !='Inshore')

#ed4.14<- droplevels(ed4.14)

n=mean(table(ed4.14$Haul_id))


#########################These are the origionals just in case something went wrong###########################
## log likelihood function for normal elements
#n=nrow(ed4.14)
# 
# ll.norm <- function(parameters, thedata)
#   {
#   ll <- (dnorm(thedata, mean=parameters[1], sd=(parameters[2]*sqrt(n)), log=FALSE))
#   return(ll)
# }
# 
# ## log likelihood function for gamma distribution elements
# ll.gam <- function(parameters, thedata)
#   {
#   ll <- (dgamma(thedata, rate=parameters[1]/parameters[2], shape=(parameters[1]^2)/parameters[2], log=FALSE))
#   return(ll)
# }
##############################################################################################################

## there are 2 sets of likelyhood funcyions:
##set 1 is non poooled variance and needs SE converted to SD
##set 2 is pooled variance and is already SD so no need to convert in the function
# #Set 1 not -pooled so in the function we change SE from the raster to var to work out rate and shape
 ll.norm <- function(parameters, thedata)
   {
   ll <- (dnorm(thedata, mean=parameters[1], sd=(parameters[2]*sqrt(n)), log=TRUE))
  return(ll)
 }
# 
# ## log likelihood function for gamma distribution elements
 ll.gam <- function(parameters, thedata)
   {
   ll <- (dgamma(thedata, rate=parameters[1]/((parameters[2]^2)*n), shape=(parameters[1]^2)/((parameters[2]^2)*n), log=TRUE))
   return(ll)
 }


# Set 2 Pooled variance - pooled var is sd so make sd to var for the shape rate calcs
#ll.norm <- function(parameters, thedata)
 # {
#  ll <- (dnorm(thedata, mean=parameters[1], sd=parameters[2], log=TRUE))
#  return(ll)
#}

## log likelihood function for gamma distribution elements
#ll.gam <- function(parameters, thedata)
#  {
#  ll <- (dgamma(thedata, rate=parameters[1]/(parameters[2]^2), shape=(parameters[1]^2)/(parameters[2]^2), #log=TRUE))
#  return(ll)
#}


# Make data larger sclae to speed up the process
# 
#   Na.big<-aggregate(Na, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
#   Naer.big<-aggregate(Na.er, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
# # # 
#   Mg.big<-aggregate(Mg, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
#   Mger.big<-aggregate(Mg.er, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
# # # 
#   Ba.big<-aggregate(Ba, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
#   Baer.big<-aggregate(Ba.er, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
# # # 
#   Sr.big<-aggregate(Sr, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
#   Srer.big<-aggregate(Sr.er, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
# # # 
#   Mn.big<-aggregate(Mn, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
#   Mner.big<-aggregate(Mn.er, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
# # # 
#   Cu.big<-aggregate(Cu, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
#   Cuer.big<-aggregate(Cu.er, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
# # # 
#   Rb.big<-aggregate(Rb, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
#   Rber.big<-aggregate(Rb.er, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
# # # 
#   Li.big<-aggregate(Li, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
#   Lier.big<-aggregate(Li.er, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
# # # 
#   Sc.big<-aggregate(Sc, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)
#   Scer.big<-aggregate(Sc.er, fact=2, fun=mean, expand=TRUE, na.rm=TRUE)

cut<- raster('F:/R_script/Microchemistry/elemental_maps/elem_layers/all_area_cut')
#plot(cut)
# ## or try at the normal scale
########note try without the pooled varience
   Na.big<-Na
   Naer.big<-Na.er
  # Naer.big<-Napool
# # # 
   Mg.big<-Mg
   Mger.big<-Mg.er
  #Mger.big<-Mgpool
# # # 
   P.big<-P
  Per.big<- P.er
  # Per.big<- Ppool
# # # 
   Ba.big<-Ba
    Baer.big<-Ba.er
  # Baer.big<-Bapool
# # # 
   Sr.big<-Sr
   Srer.big<-Sr.er
  # Srer.big<-Srpool
# #  
   Mn.big<-Mn
   Mner.big<-Mn.er
  # Mner.big<-Mnpool
# # # 
   Zn.big<-Zn
   Zner.big<-Zn.er
   #Zner.big<-Znpool
# # # 
   Rb.big<-Rb
  Rber.big<-Rb.er
   #Rber.big<-Rbpool
# # # 
   Li.big<-Li
   Lier.big<-Li.er
  # Lier.big<-Lipool




stk<-stack(Na.big, Naer.big, Mg.big, Mger.big, P.big, Per.big,  Ba.big, Baer.big, Sr.big, Srer.big,  Mn.big, Mner.big, Zn.big, Zner.big, Rb.big, Rber.big, Li.big, Lier.big) # make a raster stack form the above
# cut the stack to the smaller extent
#stk<- stk*cut
param<- as.data.frame(stk, xy=TRUE)# change the above raster back to a dataframe to compute ll

names(param)[3:20]<- c('meanNa', 'sdNa', 'meanMg', 'sdMg', 'meanP', 'sdP','meanBa', 'sdBa', 'meanSr', 'sdSr', 'meanMn', 'sdMn', 'meanZn', 'sdZn', 'meanRb', 'sdRb', 'meanLi', 'sdLi')
param<- cbind(param, ll=NA) # add the log likelyhood column
head(param)
Acc<- data.frame(ID=NA, thresh=NA, area.thresh=NA)

d=NA #vector to store geographic dists in
din=NA # vector to store geographic dists for inshore in
dll=NA # vector to store vertical distances in

samps= 1000
first.run<- TRUE

for (j in 1:samps)
{
  
## pull a sample row from data
samp<- ed4.14[sample(nrow(ed4.14), 1, replace =TRUE), ]
answer= samp$region # save the answer row for later

# make sample into an sp object and project
coords.tmp<- cbind(samp$long, samp$lat)
## create spdf object
samp.spdf<- SpatialPointsDataFrame(coords.tmp, data = data.frame(samp),
                                   proj4string = CRS(WGS84))
samp.spdfT<- spTransform(samp.spdf, CRS(mrc))


# a loop to estimate liklihood of the data given the parameters and add it to the ll column of param

  for(i in 1:nrow(param))
  
 
  {
     Na.param <- ll.norm(parameters=c(param$meanNa[i],param$sdNa[i]), thedata=samp$Na)
     Mg.param <- ll.norm(parameters=c(param$meanMg[i],param$sdMg[i]), thedata=samp$Mg)
     P.param <- ll.norm(parameters=c(param$meanP[i],param$sdP[i]), thedata=samp$P)
     Ba.param <- ll.gam(parameters=c(param$meanBa[i],param$sdBa[i]), thedata=samp$Ba)
     Sr.param <- ll.norm(parameters=c(param$meanSr[i],param$sdSr[i]), thedata=samp$Sr)
     Mn.param <- ll.gam(parameters=c(param$meanMn[i],param$sdMn[i]), thedata=samp$Mn)
     Zn.param <- ll.gam(parameters=c(param$meanZn[i],param$sdZn[i]), thedata=samp$Zn)
     Rb.param <- ll.gam(parameters=c(param$meanRb[i],param$sdRb[i]), thedata=samp$Rb)
     Li.param <- ll.gam(parameters=c(param$meanLi[i],param$sdLi[i]), thedata=samp$Li)
    
  
    # optimal sets: Cl:  Rb, Sc
    #               EIS: Sr, Rb
    #               WIS: Mg, Sr, Rb, Cu - not great though
    #               Offshore: Na Rb Sc
    #               Inshore:Na Mg Cu Rb
    
    param[i,21]<- (#Na.param +
                    #Mg.param +
                  #P.param+
                     # Ba.param+
                     Sr.param +
                      Mn.param 
                      #Zn.param +
                    # Rb.param 
                      #Li.param
                     )
  }



##turn ll into sp object
coords.tmp<- cbind(param$x, param$y)
## create spdf object
ll.spdf<- SpatialPointsDataFrame(coords.tmp, data = data.frame(param),
                                   proj4string = CRS(mrc))

#then rasterise and the ll sp object
ll.r<- raster(Na.big)
ll.r<- rasterize(ll.spdf, ll.r, field = 'll', fun = mean)
#ll.r<- exp(ll.r)
#plot the likelihood surface
image(ll.r, main=paste('blue=Truth, Circ=estimate',j, sep=" "))
points(subset(ll.spdf, ll.spdf$ll==max(ll.spdf$ll,na.rm = TRUE)),cex=6) # add circle for highest point

##save likelyhood surfaces into a folder
#writeRaster(ll.r, file=paste('F:/R_script/Microchemistry/elemental_maps/LL_Surf/allLL_414', j, sep="_"), format = 'raster', overwrite=T)


corr<- subset(ll.spdf, ll.spdf$ll==max(ll.spdf$ll,na.rm = TRUE))
corr.r<- rasterize(corr, ll.r, field='meanNa', fun=mean)
samp.r<- rasterize(samp.spdfT, ll.r, field='Na', fun=mean)

#image(corr.r, add=T, col ='black') #plot the guess
image(samp.r, add=T, col = 'blue') # plot the truth


d[j]<- dist(rbind(corr@coords,samp.spdfT@coords), method = "euclidean")
## for inshore area use
din[j]<- dist(rbind(corr@coords,mid), method = "euclidean")
dll[j]<- (maxValue(mask(ll.r, corr.r)) - maxValue(mask(ll.r, samp.r)))^2

#llScale<- scale(ll.r, center=F, scale=maxValue(ll.r))

Acc$thresh<- qnorm(
                  pnorm(extract(ll.r, samp.spdfT, cellnumbers=TRUE)[2], cellStats(ll.r, 'mean'), cellStats(ll.r, 'sd')), cellStats(ll.r, 'mean'), cellStats(ll.r, 'sd'))
Acc$ID<- as.character(samp.spdfT$ID)

fun<- function(x) {ifelse(x<Acc$thresh, NA, x)}


ll10<- calc(ll.r, fun=fun)

Acc$area.thresh<- cellStats(!is.na(ll10), 'sum')/cellStats(!is.na(ll.r), 'sum')

if (first.run)
{
 Acc.df<- Acc 
  first.run<- FALSE
} else {
  Acc.df<- rbind(Acc.df, Acc)
  
}


answer

}
Precis<- seq(from=0, to = 1, by= 0.01)
  Acur=NA
for (l in 1:length(Precis)) {
  Acur[l]<- sum(Acc.df$area.thresh< Precis[l])/nrow(Acc.df)
  
}


plot(Precis, Acur, type ='l', xlab='Precision', ylab='Accuracy', xaxt='n', ylim=c(0,1))
axis(1, at=c(0.0,0.2,0.4,0.6,0.8,1.0))
abline(a=0, b=1)
#length(which(d/1000 <= 150))/nrow(ed4.14)*100# percentage correct
##### for inshore area###
#top= cbind(x=-760757.4, y=7910812)
#mid= cbind(x=-722018.2, y=7704320)
#bot= cbind(x=-587989.6, y=7379547)
#(dist(rbind(top, bot), method = "euclidean")*0.5)/1000

length(which(d/1000 <= 100))/samps*100# percentage correct within 100km
length(which(d/1000 <= 150))/samps*100# percentage correct within 150km
length(which(d/1000 <= 175))/samps*100# percentage correct within 175km
#length(which(d/1000 <= 200))/samps*100# percentage correct within 200km
mean(d)/1000
#max(d)/1000
flux::auc(Precis, Acur)
## AUC was 0.95


theta<- Acur + rev(Precis)
which(theta==max(theta), arr.ind=T)
n <- length(theta)
max<- sort(theta)[n]


x.tmp<-Precis[which(theta==max, arr.ind=T)]
y.tmp<- Acur[which(theta==max, arr.ind=T)]
points(x.tmp, y.tmp, col='green')

accs<- cbind(Precis, Acur)
#write.csv(accs, 'F:/R_script/Microchemistry/Elemental_maps/accs414.csv')
 ## use this file - open in excell match the closest accuracy figure to percentage 80 and 90%  and add to below code
#abline(a=0.7, b=1)

### look to see what precision is at 90% acc and 80% acc
accs414<-read.csv('F:/R_script/Microchemistry/Elemental_maps/accs414.csv')

#75%
#Precis[which(Acur==0.7, arr.ind=T)]

int<- accs414$Acur[which.min(abs(accs414$Acur - 0.75))]
accs414$Precis[which(accs414$Acur==int, arr.ind=T)]
# answer = 0.02

#80%
#Precis[which(Acur==0.799, arr.ind=T)]

int<- accs414$Acur[which.min(abs(accs414$Acur - 0.8))]
accs414$Precis[which(accs414$Acur==int, arr.ind=T)]

#anwer = 0.03

#90%
#Precis[which(Acur==0.897, arr.ind=T)]

int<- accs414$Acur[which.min(abs(accs414$Acur - 0.9))]
accs414$Precis[which(accs414$Acur==int, arr.ind=T)]
 # answer = 0.09

#rms<- sqrt(mean(dll))
#rms


#length(which(din/1000 <= 280))/samps*100# percentage correct ### for inshore

# mean(d)/1000
# max(d)/1000
# rms<- sqrt(mean(dll))
# rms
# 
# head(Acc.df)
# 
# dists<- c(1:400)
# per<-NA
# 
# 
# for (z in 1:length(dists)){
#  per[z]<- length(which(d/1000 <= z))/samps*100# percentage correct within 150km
# }
# plot(dists, per)
# 
# length(which(d/1000 <= 175))/samps*100# percentage correct within 150km

```


#plot of acc vs prec
```{r}

Precis<- seq(from=0, to = 1, by= 0.01)
  Acur=NA
for (l in 1:length(Precis)) {
  Acur[l]<- sum(Acc.df$area.thresh< Precis[l])/nrow(Acc.df)
  
}


plot(1-Precis, 1-Acur, type ='l', xlab='Precision', ylab='Accuracy', xaxt='n')
axis(1, at=c(0.0,0.2,0.4,0.6,0.8,1.0))
abline(a=0, b=1)

flux::auc(Precis, Acur)
## theta= the value or precis + acur. The maximum theta = optimal vaue to use as a threshold
theta<-  Acur + rev(Precis)
which(theta==max(theta), arr.ind=T)
n <- length(theta)
max<- sort(theta)[n]
#max<-sort(theta,partial=n-1)[n-1]

x.tmp<-Precis[which(theta==max, arr.ind=T)]
y.tmp<- Acur[which(theta==max, arr.ind=T)]
points(x.tmp, y.tmp, col='green')

accs<- cbind(Precis, Acur)
#write.csv(accs, 'F:/R_script/Microchemistry/Elemental_maps/accs414.csv')
 ## use this file - open in excell match the closest accuracy figure to percentage 80 and 90%  and add to below code
abline(a=0.7, b=1)

### look to see what precision is at 90% acc and 80% acc

#75%
Precis[which(Acur==0.763, arr.ind=T)]
#80%
Precis[which(Acur==0.799, arr.ind=T)]

#90%
Precis[which(Acur==0.897, arr.ind=T)]


```


#end
